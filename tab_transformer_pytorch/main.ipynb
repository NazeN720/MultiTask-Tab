{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d92b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_transformer_pytorch import TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76d5b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a2eea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    \"\"\"Feature Tokenizer Transformer - Base model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        categories: List[int],\n",
    "        num_continuous: int,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        heads: int,\n",
    "        dim_out: int,\n",
    "        attn_dropout: float = 0.1,\n",
    "        ff_dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.categories = categories\n",
    "        self.num_continuous = num_continuous\n",
    "        self.dim = dim\n",
    "        self.dim_out = dim_out\n",
    "        \n",
    "        # Categorical embeddings\n",
    "        self.categorical_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_size, dim) for cat_size in categories\n",
    "        ])\n",
    "        \n",
    "        # Continuous feature projection\n",
    "        if num_continuous > 0:\n",
    "            self.continuous_projection = nn.Linear(num_continuous, dim)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        total_tokens = len(categories) + (1 if num_continuous > 0 else 0)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(total_tokens, dim))\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(dim, heads, attn_dropout, ff_dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Output projection to get desired embedding dimension\n",
    "        self.output_projection = nn.Linear(dim, dim_out)\n",
    "        \n",
    "    def forward(self, x_categorical: torch.Tensor, x_continuous: Optional[torch.Tensor] = None):\n",
    "        batch_size = x_categorical.size(0)\n",
    "        tokens = []\n",
    "        \n",
    "        # Process categorical features\n",
    "        for i, embedding in enumerate(self.categorical_embeddings):\n",
    "            cat_token = embedding(x_categorical[:, i])\n",
    "            tokens.append(cat_token)\n",
    "        \n",
    "        # Process continuous features\n",
    "        if x_continuous is not None and self.num_continuous > 0:\n",
    "            cont_token = self.continuous_projection(x_continuous)\n",
    "            tokens.append(cont_token)\n",
    "        \n",
    "        # Stack tokens and add positional embeddings\n",
    "        x = torch.stack(tokens, dim=1)  # [batch_size, num_tokens, dim]\n",
    "        x = x + self.pos_embedding.unsqueeze(0)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply layer norm\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        # Global average pooling across tokens\n",
    "        x = x.mean(dim=1)  # [batch_size, dim]\n",
    "        \n",
    "        # Project to output dimension\n",
    "        x = self.output_projection(x)  # [batch_size, dim_out]\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with multi-head attention and feed-forward\"\"\"\n",
    "    def __init__(self, dim: int, heads: int, attn_dropout: float = 0.1, ff_dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=heads,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(ff_dropout),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(ff_dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    \"\"\"Task-specific head for different types of outputs\"\"\"\n",
    "    def __init__(self, input_dim: int, task_config: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        self.task_type = task_config['type']\n",
    "        \n",
    "        if self.task_type == 'regression':\n",
    "            output_dim = task_config.get('output_dim', 1)\n",
    "            # Add output scaling for better initialization\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(input_dim // 2, output_dim)\n",
    "            )\n",
    "            \n",
    "            # Optional: Add output scaling parameters\n",
    "            self.output_scale = task_config.get('output_scale', 100)\n",
    "            self.output_bias = task_config.get('output_bias', 150)\n",
    "            \n",
    "        elif self.task_type == 'classification':\n",
    "            num_classes = task_config['num_classes']\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(input_dim // 2, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task type: {self.task_type}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.head(x)\n",
    "        \n",
    "        # Apply scaling for regression tasks\n",
    "        if self.task_type == 'regression':\n",
    "            output = output * self.output_scale + self.output_bias\n",
    "            \n",
    "        return output\n",
    "\n",
    "\n",
    "class MultitaskFTTransformer(nn.Module):\n",
    "    \"\"\"Multitask wrapper for FT Transformer\"\"\"\n",
    "    def __init__(self, base_model: FTTransformer, task_configs: Dict[str, Dict[str, Any]]):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.task_configs = task_configs\n",
    "        \n",
    "        # Create task-specific heads\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            task_name: TaskHead(base_model.dim_out, config)\n",
    "            for task_name, config in task_configs.items()\n",
    "        })\n",
    "    \n",
    "    def forward(self, x_categorical: torch.Tensor, x_continuous: Optional[torch.Tensor] = None, \n",
    "                tasks: Optional[List[str]] = None):\n",
    "        # Get shared representation from base model\n",
    "        shared_repr = self.base_model(x_categorical, x_continuous)\n",
    "        \n",
    "        # Apply task-specific heads\n",
    "        if tasks is None:\n",
    "            tasks = list(self.task_heads.keys())\n",
    "        \n",
    "        outputs = {}\n",
    "        for task_name in tasks:\n",
    "            if task_name in self.task_heads:\n",
    "                outputs[task_name] = self.task_heads[task_name](shared_repr)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "def create_multitask_loss(outputs: Dict[str, torch.Tensor], \n",
    "                         targets: Dict[str, torch.Tensor],\n",
    "                         task_configs: Dict[str, Dict[str, Any]],\n",
    "                         task_weights: Optional[Dict[str, float]] = None) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Create multitask loss function\"\"\"\n",
    "    if task_weights is None:\n",
    "        task_weights = {task: 1.0 for task in task_configs.keys()}\n",
    "    \n",
    "    losses = {}\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for task_name, config in task_configs.items():\n",
    "        if task_name not in outputs or task_name not in targets:\n",
    "            continue\n",
    "            \n",
    "        pred = outputs[task_name]\n",
    "        target = targets[task_name]\n",
    "        weight = task_weights.get(task_name, 1.0)\n",
    "        \n",
    "        if config['type'] == 'regression':\n",
    "            loss = F.mse_loss(pred, target)\n",
    "        elif config['type'] == 'classification':\n",
    "            loss = F.cross_entropy(pred, target)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task type: {config['type']}\")\n",
    "        \n",
    "        losses[task_name] = loss\n",
    "        total_loss += weight * loss\n",
    "    \n",
    "    losses['total'] = total_loss\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Advanced features for better multitask learning\n",
    "class AdaptiveTaskWeighting(nn.Module):\n",
    "    \"\"\"Adaptive task weighting based on uncertainty\"\"\"\n",
    "    def __init__(self, task_names: List[str]):\n",
    "        super().__init__()\n",
    "        self.task_names = task_names\n",
    "        # Learnable log variance for each task\n",
    "        self.log_vars = nn.Parameter(torch.zeros(len(task_names)))\n",
    "    \n",
    "    def forward(self, losses: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        total_loss = 0.0\n",
    "        for i, task_name in enumerate(self.task_names):\n",
    "            if task_name in losses:\n",
    "                # Uncertainty weighting: 1/(2*sigma^2) * loss + log(sigma)\n",
    "                precision = torch.exp(-self.log_vars[i])\n",
    "                total_loss += precision * losses[task_name] + self.log_vars[i]\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class GradientBalancing:\n",
    "    \"\"\"Gradient balancing for multitask learning\"\"\"\n",
    "    def __init__(self, task_names: List[str], alpha: float = 0.12):\n",
    "        self.task_names = task_names\n",
    "        self.alpha = alpha\n",
    "        self.task_losses_history = {task: [] for task in task_names}\n",
    "    \n",
    "    def compute_weights(self, current_losses: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
    "        weights = {}\n",
    "        \n",
    "        for task_name in self.task_names:\n",
    "            if task_name in current_losses:\n",
    "                # Add current loss to history\n",
    "                self.task_losses_history[task_name].append(current_losses[task_name].item())\n",
    "                \n",
    "                # Keep only recent history\n",
    "                if len(self.task_losses_history[task_name]) > 100:\n",
    "                    self.task_losses_history[task_name].pop(0)\n",
    "                \n",
    "                # Compute relative loss rate\n",
    "                if len(self.task_losses_history[task_name]) > 1:\n",
    "                    recent_avg = sum(self.task_losses_history[task_name][-10:]) / min(10, len(self.task_losses_history[task_name]))\n",
    "                    overall_avg = sum(self.task_losses_history[task_name]) / len(self.task_losses_history[task_name])\n",
    "                    rate = recent_avg / (overall_avg + 1e-8)\n",
    "                    weights[task_name] = rate ** self.alpha\n",
    "                else:\n",
    "                    weights[task_name] = 1.0\n",
    "        \n",
    "        return weights\n",
    "\n",
    "\n",
    "# Enhanced usage example\n",
    "def train_multitask_model():\n",
    "    \"\"\"Complete training example with advanced features\"\"\"\n",
    "    \n",
    "    # Model setup\n",
    "    base_model = FTTransformer(\n",
    "        categories=[10, 5, 6],\n",
    "        num_continuous=8,\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        dim_out=128\n",
    "    )\n",
    "    \n",
    "    task_configs = {\n",
    "        'price': {'type': 'regression', 'output_dim': 1},\n",
    "        'category': {'type': 'classification', 'num_classes': 5},\n",
    "    }\n",
    "    \n",
    "    model = MultitaskFTTransformer(base_model, task_configs)\n",
    "    \n",
    "    # Advanced loss components\n",
    "    adaptive_weighting = AdaptiveTaskWeighting(list(task_configs.keys()))\n",
    "    gradient_balancing = GradientBalancing(list(task_configs.keys()))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(model.parameters()) + list(adaptive_weighting.parameters()),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # Training loop example\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        # Generate dummy batch\n",
    "        batch_size = 32\n",
    "        x_categorical = torch.randint(0, 5, (batch_size, 3))\n",
    "        x_continuous = torch.randn(batch_size, 8)\n",
    "        \n",
    "        targets = {\n",
    "            'price': torch.randn(batch_size, 1) * 50 + 150,  # Price range: ~100-200\n",
    "            'category': torch.randint(0, 5, (batch_size,)),   # Categories: 0-4\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_categorical, x_continuous)\n",
    "        \n",
    "        # Calculate individual task losses\n",
    "        task_losses = {}\n",
    "        for task_name, config in task_configs.items():\n",
    "            pred = outputs[task_name]\n",
    "            target = targets[task_name]\n",
    "            \n",
    "            if config['type'] == 'regression':\n",
    "                task_losses[task_name] = F.mse_loss(pred, target)\n",
    "            elif config['type'] == 'classification':\n",
    "                task_losses[task_name] = F.cross_entropy(pred, target)\n",
    "        \n",
    "        # Apply adaptive weighting\n",
    "        total_loss = adaptive_weighting(task_losses)\n",
    "        \n",
    "        # Alternative: Use gradient balancing\n",
    "        # weights = gradient_balancing.compute_weights(task_losses)\n",
    "        # total_loss = sum(weights[task] * loss for task, loss in task_losses.items())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch {epoch}: Total Loss = {total_loss.item():.4f}\")\n",
    "            for task_name, loss in task_losses.items():\n",
    "                print(f\"  {task_name}: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Inference utilities\n",
    "class MultitaskInference:\n",
    "    \"\"\"Utilities for inference with multitask model\"\"\"\n",
    "    def __init__(self, model: MultitaskFTTransformer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict_single_task(self, x_categorical: torch.Tensor, \n",
    "                           x_continuous: Optional[torch.Tensor], \n",
    "                           task_name: str) -> torch.Tensor:\n",
    "        \"\"\"Predict for a single task only\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(x_categorical, x_continuous, tasks=[task_name])\n",
    "            return outputs[task_name]\n",
    "    \n",
    "    def predict_all_tasks(self, x_categorical: torch.Tensor, \n",
    "                         x_continuous: Optional[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Predict for all tasks\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model(x_categorical, x_continuous)\n",
    "    \n",
    "    def get_embeddings(self, x_categorical: torch.Tensor, \n",
    "                      x_continuous: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Get shared embeddings from base model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.model.base_model(x_categorical, x_continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8c22a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7e82a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\rayhn\\College\\MK Semester 7\\Tugas Akhir 1\\MY SKRIPSI GUE\\MultiTask-Tab\\data.csv', delimiter=';',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7c954a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Lapse'] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26fc92cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lapse\n",
       "0    84007\n",
       "1    20008\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Lapse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06b7eac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Date_start_contract', 'Date_last_renewal', 'Date_next_renewal',\n",
       "       'Date_birth', 'Date_driving_licence', 'Distribution_channel',\n",
       "       'Seniority', 'Policies_in_force', 'Max_policies', 'Max_products',\n",
       "       'Lapse', 'Date_lapse', 'Payment', 'Premium', 'Cost_claims_year',\n",
       "       'N_claims_year', 'N_claims_history', 'R_Claims_history', 'Type_risk',\n",
       "       'Area', 'Second_driver', 'Year_matriculation', 'Power',\n",
       "       'Cylinder_capacity', 'Value_vehicle', 'N_doors', 'Type_fuel', 'Length',\n",
       "       'Weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc6ebf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 104015 entries, 0 to 105554\n",
      "Data columns (total 30 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   ID                    104015 non-null  int64  \n",
      " 1   Date_start_contract   104015 non-null  object \n",
      " 2   Date_last_renewal     104015 non-null  object \n",
      " 3   Date_next_renewal     104015 non-null  object \n",
      " 4   Date_birth            104015 non-null  object \n",
      " 5   Date_driving_licence  104015 non-null  object \n",
      " 6   Distribution_channel  104015 non-null  int64  \n",
      " 7   Seniority             104015 non-null  int64  \n",
      " 8   Policies_in_force     104015 non-null  int64  \n",
      " 9   Max_policies          104015 non-null  int64  \n",
      " 10  Max_products          104015 non-null  int64  \n",
      " 11  Lapse                 104015 non-null  int64  \n",
      " 12  Date_lapse            33790 non-null   object \n",
      " 13  Payment               104015 non-null  int64  \n",
      " 14  Premium               104015 non-null  float64\n",
      " 15  Cost_claims_year      104015 non-null  float64\n",
      " 16  N_claims_year         104015 non-null  int64  \n",
      " 17  N_claims_history      104015 non-null  int64  \n",
      " 18  R_Claims_history      104015 non-null  float64\n",
      " 19  Type_risk             104015 non-null  int64  \n",
      " 20  Area                  104015 non-null  int64  \n",
      " 21  Second_driver         104015 non-null  int64  \n",
      " 22  Year_matriculation    104015 non-null  int64  \n",
      " 23  Power                 104015 non-null  int64  \n",
      " 24  Cylinder_capacity     104015 non-null  int64  \n",
      " 25  Value_vehicle         104015 non-null  float64\n",
      " 26  N_doors               104015 non-null  int64  \n",
      " 27  Type_fuel             102341 non-null  object \n",
      " 28  Length                93922 non-null   float64\n",
      " 29  Weight                104015 non-null  int64  \n",
      "dtypes: float64(5), int64(18), object(7)\n",
      "memory usage: 24.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15f468d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lapse\n",
      "0    20000\n",
      "1    20000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ambil 500 sampel dari tiap kelas\n",
    "data = (\n",
    "    data.groupby(\"Lapse\", group_keys=False)\n",
    "        .apply(lambda x: x.sample(n=20000, random_state=42))\n",
    ")\n",
    "\n",
    "print(data[\"Lapse\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dafac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40511d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af136c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_multitask_model():\n",
    "#     \"\"\"Complete training example with advanced features\"\"\"\n",
    "    \n",
    "#     # Model setup\n",
    "#     cat_cols = ['Seniority', 'Policies_in_force', 'Max_policies', 'Max_products', 'Type_risk']\n",
    "#     num_cols = ['Cylinder_capacity', 'Power', 'Year_matriculation']\n",
    "#     num_categories = [data[col].nunique() for col in cat_cols]\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "\n",
    "#     from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#     for col in cat_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         data[col] = le.fit_transform(data[col])\n",
    "\n",
    "\n",
    "#     base_model = FTTransformer(\n",
    "#         categories=num_categories,\n",
    "#         num_continuous=len(num_cols),\n",
    "#         dim=128,\n",
    "#         depth=6,\n",
    "#         heads=8,\n",
    "#         dim_out=128\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     task_configs = {\n",
    "#         'Value_vehicle': {'type': 'regression', 'output_dim': 1},\n",
    "#         'lapse': {'type': 'classification', 'num_classes': 2},\n",
    "#     }\n",
    "    \n",
    "#     model = MultitaskFTTransformer(base_model, task_configs)\n",
    "    \n",
    "#     # Advanced loss components\n",
    "#     adaptive_weighting = AdaptiveTaskWeighting(list(task_configs.keys()))\n",
    "#     gradient_balancing = GradientBalancing(list(task_configs.keys()))\n",
    "    \n",
    "#     # Optimizer\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         list(model.parameters()) + list(adaptive_weighting.parameters()),\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "    \n",
    "#     # Training loop example\n",
    "#     model.train()\n",
    "#     for epoch in range(3):\n",
    "#         # Generate dummy batch\n",
    "\n",
    "#         x_categorical = torch.tensor(\n",
    "#             data[cat_cols].astype('int64').values, dtype=torch.long\n",
    "#         )\n",
    "\n",
    "#         x_continuous = torch.tensor(\n",
    "#             data[num_cols].astype('float32').values, dtype=torch.float32\n",
    "#         )\n",
    "\n",
    "#         targets = {\n",
    "#             'Value_vehicle': torch.tensor(\n",
    "#                 data['Value_vehicle'].astype('float32').values, dtype=torch.float32\n",
    "#             ).unsqueeze(1),  # regression tetap (N,1)\n",
    "#             'lapse': torch.tensor(\n",
    "#                 data['Lapse'].astype('int64').values, dtype=torch.long\n",
    "#             ),  # classification harus 1D (N,)\n",
    "#         }\n",
    "\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(x_categorical, x_continuous)\n",
    "        \n",
    "#         # Calculate individual task losses\n",
    "#         task_losses = {}\n",
    "#         for task_name, config in task_configs.items():\n",
    "#             pred = outputs[task_name]\n",
    "#             target = targets[task_name]\n",
    "            \n",
    "#             if config['type'] == 'regression':\n",
    "#                 task_losses[task_name] = F.mse_loss(pred, target)\n",
    "#             elif config['type'] == 'classification':\n",
    "#                 task_losses[task_name] = F.cross_entropy(pred, target)\n",
    "        \n",
    "#         # Apply adaptive weighting\n",
    "        \n",
    "#         # Alternative: Use gradient balancing\n",
    "#         weights = gradient_balancing.compute_weights(task_losses)\n",
    "#         total_loss = sum(weights[task] * loss for task, loss in task_losses.items())\n",
    "        \n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "        \n",
    "#         # Gradient clipping\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if epoch % 2 == 0:\n",
    "#             print(f\"Epoch {epoch}: Total Loss = {total_loss.item():.4f}\")\n",
    "#             for task_name, loss in task_losses.items():\n",
    "#                 print(f\"  {task_name}: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10bd9641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value_vehicle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18280.731560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9118.739966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>270.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17480.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22466.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>220675.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Value_vehicle\n",
       "count   40000.000000\n",
       "mean    18280.731560\n",
       "std      9118.739966\n",
       "min       270.460000\n",
       "25%     12950.000000\n",
       "50%     17480.000000\n",
       "75%     22466.287500\n",
       "max    220675.800000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Value_vehicle']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08a42dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total Loss = 27549.7246\n",
      "  Value_vehicle: 27549.0234\n",
      "  lapse: 0.7016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model setup\n",
    "cat_cols = ['Seniority', 'Policies_in_force', 'Max_policies', 'Max_products', 'Type_risk']\n",
    "num_cols = ['Cylinder_capacity', 'Power', 'Year_matriculation']\n",
    "num_categories = [data[col].nunique() for col in cat_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# data[['Value_vehicle']] = scaler_y.fit_transform(data[['Value_vehicle']])\n",
    "\n",
    "data[['Value_vehicle']] = data[['Value_vehicle']] / 16500\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "\n",
    "base_model = FTTransformer(\n",
    "    categories=num_categories,\n",
    "    num_continuous=len(num_cols),\n",
    "    dim=32,\n",
    "    depth=3,\n",
    "    heads=8,\n",
    "    dim_out=64\n",
    ")\n",
    "\n",
    "\n",
    "task_configs = {\n",
    "    'Value_vehicle': {'type': 'regression', 'output_dim': 1},\n",
    "    'lapse': {'type': 'classification', 'num_classes': 2},\n",
    "}\n",
    "\n",
    "model = MultitaskFTTransformer(base_model, task_configs)\n",
    "\n",
    "# Advanced loss components\n",
    "adaptive_weighting = AdaptiveTaskWeighting(list(task_configs.keys()))\n",
    "gradient_balancing = GradientBalancing(list(task_configs.keys()))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()) + list(adaptive_weighting.parameters()),\n",
    "    lr=0.0001,\n",
    "    weight_decay=0\n",
    ")\n",
    "\n",
    "# Training loop example\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    # Generate dummy batch\n",
    "\n",
    "    x_categorical = torch.tensor(\n",
    "        data[cat_cols].astype('int64').values, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    x_continuous = torch.tensor(\n",
    "        data[num_cols].astype('float32').values, dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    targets = {\n",
    "        'Value_vehicle': torch.tensor(\n",
    "        data['Value_vehicle'].astype('float32').values, dtype=torch.float32\n",
    "        ).unsqueeze(1),  # regression tetap (N,1)\n",
    "        'lapse': torch.tensor(\n",
    "            data['Lapse'].astype('int64').values, dtype=torch.long\n",
    "        ),  # classification harus 1D (N,)\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(x_categorical, x_continuous)\n",
    "    \n",
    "    # Calculate individual task losses\n",
    "    task_losses = {}\n",
    "    for task_name, config in task_configs.items():\n",
    "        pred = outputs[task_name]\n",
    "        target = targets[task_name]\n",
    "        \n",
    "        if config['type'] == 'regression':\n",
    "            task_losses[task_name] = F.mse_loss(pred, target)\n",
    "        elif config['type'] == 'classification':\n",
    "            task_losses[task_name] = F.cross_entropy(pred, target)\n",
    "    \n",
    "    # Apply adaptive weighting\n",
    "    \n",
    "    # Alternative: Use gradient balancing\n",
    "    weights = gradient_balancing.compute_weights(task_losses)\n",
    "    total_loss = sum(weights[task] * loss for task, loss in task_losses.items())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss = {total_loss.item():.4f}\")\n",
    "        for task_name, loss in task_losses.items():\n",
    "            print(f\"  {task_name}: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db7c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total Loss = 26932.1152\n",
      "  Value_vehicle: 26931.4141\n",
      "    RMSE     : 164.1079\n",
      "  lapse: 0.7016\n",
      "    Accuracy : 0.4961\n",
      "Epoch 2: Total Loss = 26311.0176\n",
      "  Value_vehicle: 26310.3164\n",
      "    RMSE     : 162.2045\n",
      "  lapse: 0.7003\n",
      "    Accuracy : 0.4952\n",
      "Epoch 4: Total Loss = 25687.5508\n",
      "  Value_vehicle: 25686.8496\n",
      "    RMSE     : 160.2712\n",
      "  lapse: 0.7004\n",
      "    Accuracy : 0.4973\n",
      "Epoch 6: Total Loss = 25069.4043\n",
      "  Value_vehicle: 25068.7031\n",
      "    RMSE     : 158.3310\n",
      "  lapse: 0.7004\n",
      "    Accuracy : 0.4944\n",
      "Epoch 8: Total Loss = 24365.1875\n",
      "  Value_vehicle: 24382.0547\n",
      "    RMSE     : 156.1475\n",
      "  lapse: 0.6995\n",
      "    Accuracy : 0.4965\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10):\n",
    "    # Generate dummy batch\n",
    "    x_categorical = torch.tensor(\n",
    "        data[cat_cols].astype('int64').values, dtype=torch.long\n",
    "    )\n",
    "    x_continuous = torch.tensor(\n",
    "        data[num_cols].astype('float32').values, dtype=torch.float32\n",
    "    )\n",
    "    targets = {\n",
    "        'Value_vehicle': torch.tensor(\n",
    "            data['Value_vehicle'].astype('float32').values, dtype=torch.float32\n",
    "        ).unsqueeze(1),  # regression tetap (N,1)\n",
    "        'lapse': torch.tensor(\n",
    "            data['Lapse'].astype('int64').values, dtype=torch.long\n",
    "        ),  # classification harus 1D (N,)\n",
    "    }\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(x_categorical, x_continuous)\n",
    "\n",
    "    # Calculate individual task losses\n",
    "    task_losses = {}\n",
    "    metrics = {}\n",
    "    for task_name, config in task_configs.items():\n",
    "        pred = outputs[task_name]\n",
    "        target = targets[task_name]\n",
    "\n",
    "        if config['type'] == 'regression':\n",
    "            task_losses[task_name] = F.mse_loss(pred, target)\n",
    "            # --- RMSE ---\n",
    "            rmse = torch.sqrt(task_losses[task_name]).item()\n",
    "            metrics[task_name] = {\"rmse\": rmse}\n",
    "\n",
    "        elif config['type'] == 'classification':\n",
    "            task_losses[task_name] = F.cross_entropy(pred, target)\n",
    "            # --- Accuracy ---\n",
    "            pred_labels = pred.argmax(dim=1)\n",
    "            acc = (pred_labels == target).float().mean().item()\n",
    "            metrics[task_name] = {\"accuracy\": acc}\n",
    "\n",
    "    # Adaptive weighting\n",
    "    weights = gradient_balancing.compute_weights(task_losses)\n",
    "    total_loss = sum(weights[task] * loss for task, loss in task_losses.items())\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss = {total_loss.item():.4f}\")\n",
    "        for task_name, loss in task_losses.items():\n",
    "            print(f\"  {task_name}: {loss.item():.4f}\")\n",
    "            if 'rmse' in metrics[task_name]:\n",
    "                print(f\"    RMSE     : {metrics[task_name]['rmse']:.4f}\")\n",
    "            if 'accuracy' in metrics[task_name]:\n",
    "                print(f\"    Accuracy : {metrics[task_name]['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e7754dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = FTTransformer(\n",
    "    categories=num_categories,\n",
    "    num_continuous=len(num_cols),\n",
    "    dim=32,\n",
    "    depth=3,\n",
    "    heads=8,\n",
    "    dim_out=64\n",
    ")\n",
    "\n",
    "\n",
    "task_configs = {\n",
    "    'Value_vehicle': {'type': 'regression', 'output_dim': 1},\n",
    "    'lapse': {'type': 'classification', 'num_classes': 2},\n",
    "}\n",
    "\n",
    "model = MultitaskFTTransformer(base_model, task_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "268e3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_categorical = torch.tensor(\n",
    "    data[cat_cols].astype('int64').values, dtype=torch.long\n",
    ")\n",
    "x_continuous = torch.tensor(\n",
    "    data[num_cols].astype('float32').values, dtype=torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e9a00387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MultitaskFTTransformer                        [40000, 2]                --\n",
       "├─FTTransformer: 1-1                          [40000, 64]               192\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─Embedding: 3-1                    [40000, 32]               1,280\n",
       "│    │    └─Embedding: 3-2                    [40000, 32]               512\n",
       "│    │    └─Embedding: 3-3                    [40000, 32]               480\n",
       "│    │    └─Embedding: 3-4                    [40000, 32]               128\n",
       "│    │    └─Embedding: 3-5                    [40000, 32]               128\n",
       "│    └─Linear: 2-2                            [40000, 32]               128\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─TransformerBlock: 3-6             [40000, 6, 32]            12,704\n",
       "│    │    └─TransformerBlock: 3-7             [40000, 6, 32]            12,704\n",
       "│    │    └─TransformerBlock: 3-8             [40000, 6, 32]            12,704\n",
       "│    └─LayerNorm: 2-4                         [40000, 6, 32]            64\n",
       "│    └─Linear: 2-5                            [40000, 64]               2,112\n",
       "├─ModuleDict: 1-2                             --                        --\n",
       "│    └─TaskHead: 2-6                          [40000, 1]                --\n",
       "│    │    └─Sequential: 3-9                   [40000, 1]                2,113\n",
       "│    └─TaskHead: 2-7                          [40000, 2]                --\n",
       "│    │    └─Sequential: 3-10                  [40000, 2]                2,146\n",
       "===============================================================================================\n",
       "Total params: 47,395\n",
       "Trainable params: 47,395\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.38\n",
       "===============================================================================================\n",
       "Input size (MB): 2.08\n",
       "Forward/backward pass size (MB): 1455.04\n",
       "Params size (MB): 0.14\n",
       "Estimated Total Size (MB): 1457.26\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Jika model membutuhkan dua input\n",
    "summary(model, input_data=[x_categorical, x_continuous])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ae76943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torchviz) (2.7.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from torch->torchviz) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rayhn\\anaconda3\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73c03b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_dot() got an unexpected keyword argument 'graph_attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m output_tensor = torch.cat(\u001b[38;5;28mlist\u001b[39m(y.values()), dim=\u001b[32m1\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ----- Buat graph landscape -----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m dot = make_dot(\n\u001b[32m     12\u001b[39m     output_tensor,\n\u001b[32m     13\u001b[39m     params=\u001b[38;5;28mdict\u001b[39m(model.named_parameters()),\n\u001b[32m     14\u001b[39m     graph_attr={\u001b[33m'\u001b[39m\u001b[33mrankdir\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLR\u001b[39m\u001b[33m'\u001b[39m}  \u001b[38;5;66;03m# LR = left-to-right (landscape)\u001b[39;00m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m dot.format = \u001b[33m'\u001b[39m\u001b[33mpng\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m dot.render(\u001b[33m'\u001b[39m\u001b[33mfttransformer_landscape\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: make_dot() got an unexpected keyword argument 'graph_attr'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# ----- Forward pass -----\n",
    "y = model(x_categorical, x_continuous)\n",
    "\n",
    "# Jika output dict, gabungkan semua tensor menjadi satu\n",
    "output_tensor = torch.cat(list(y.values()), dim=1)\n",
    "\n",
    "# ----- Buat graph landscape -----\n",
    "dot = make_dot(\n",
    "    output_tensor,\n",
    "    params=dict(model.named_parameters()),\n",
    "    graph_attr={'rankdir': 'LR'}  # LR = left-to-right (landscape)\n",
    ")\n",
    "\n",
    "dot.format = 'png'\n",
    "dot.render('fttransformer_landscape')\n",
    "\n",
    "print(\"Diagram FTTransformer tersimpan sebagai fttransformer_landscape.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b17cde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultitaskFTTransformer(\n",
       "  (base_model): FTTransformer(\n",
       "    (categorical_embeddings): ModuleList(\n",
       "      (0): Embedding(40, 32)\n",
       "      (1): Embedding(16, 32)\n",
       "      (2): Embedding(15, 32)\n",
       "      (3-4): 2 x Embedding(4, 32)\n",
       "    )\n",
       "    (continuous_projection): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (transformer_layers): ModuleList(\n",
       "      (0-2): 3 x TransformerBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_projection): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       "  (task_heads): ModuleDict(\n",
       "    (Value_vehicle): TaskHead(\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (lapse): TaskHead(\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=32, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e13850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price predictions: tensor([121.0222, 122.4365, 119.9268,  ..., 122.0918, 125.3112, 117.6130])\n",
      "All predictions: ['Value_vehicle', 'lapse']\n",
      "Embeddings shape: torch.Size([40000, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Single task prediction\n",
    "price_pred = MultitaskInference(model=model).predict_single_task(x_categorical=x_categorical, \n",
    "                                                    x_continuous=x_continuous, task_name='Value_vehicle')\n",
    "print(f\"Price predictions: {price_pred.flatten()}\")\n",
    "\n",
    "# All tasks prediction\n",
    "all_preds = MultitaskInference(model=model).predict_all_tasks(x_categorical=x_categorical, \n",
    "                                                    x_continuous=x_continuous,)\n",
    "print(f\"All predictions: {list(all_preds.keys())}\")\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = MultitaskInference(model=model).get_embeddings(x_categorical=x_categorical, \n",
    "                                                    x_continuous=x_continuous,)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78fe9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])  # shape (2,3)\n",
    "print(x.shape)  # (2,3)\n",
    "\n",
    "x0 = x.unsqueeze(0)\n",
    "print(x0.shape)  # (1,2,3)\n",
    "\n",
    "x1 = x.unsqueeze(1)\n",
    "print(x1.shape)  # (2,1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6240ace3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f849e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
